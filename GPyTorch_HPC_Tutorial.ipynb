{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPyTorch: Gaussian Process Modeling  At HPC Scales\n",
    "\n",
    "### Authors/Presenters: Tiffany Christian (Northwestern), Yuyang (Edward) Tiang (University of Chicago), Marieme Ngom (ANL), Carlo Graziani (ANL) \n",
    "### Other Science Authorship Credit: Julie Bessac (ANL/Virginia Tech), Vishwas Rao (ANL), Brandi Gamelin (ANL)\n",
    "\n",
    "The target audience of this tutorial is scientists who are interested in the possibility of carrying out Gaussian Process (GP) modeling at very large scales. \n",
    "\n",
    "This possibility is novel and exciting, because while GPs have always been attractive as a data modeling choice for their conceptual clarity, great flexibility, and mathematical rigor, they have has suffered from severe scalability limitations. With $N$ training samples, a \"classical\" GP treatment yields a storage cost that scales as $\\mathcal{O}(N^2)$, and a time to solution that scales as $\\mathcal{O}(N^3)$. These scalings have kept GP modeling from achieving the kind of success at large data scales that deep learning methods have attained.  \n",
    "\n",
    "This situation has changed recently in consequence of two developments. One is the the rise of fast and accurate approximation methods that scale essentially as $\\mathcal{O}(N^0)$ --- constant time! The other is the transparent implementation of such methods in a library --- $\\texttt{GPyTorch}$ --- that sits on top of $\\texttt{PyTorch}$. The reason this latter factor is significant is \n",
    "* $\\texttt{PyTorch}$ provides built-in GPU acceleration to any library that is built on top of it, if GPU hardware is locally available; and \n",
    "* $\\texttt{PyTorch}$ is one of the first libraries deployed to any scientific software stack on a new HPC platform  \n",
    "As a consequence, $\\texttt{GPyTorch}$ not only benefits from GPU acceleration effortlessly where a GPU is available, but it also gets a free ride onto any new HPC platform with a hetereogeneous architecture --- which is to say, any new HPC platform.  \n",
    "\n",
    "The $\\texttt{GPyTorch}$ documentation website is at [https://docs.gpytorch.ai/en/stable/index.html](https://docs.gpytorch.ai/en/stable/index.html), and the main website, including developer credits, is at [https://gpytorch.ai/](https://gpytorch.ai/). The doc websites contains several introductory tutorials, and an extensive API documentation system. The main website links to the github development repo, which among other things hosts an active discussion/questions site.\n",
    "\n",
    "In this tutorial, we will assume some basic knowledge of GP theory, and introduce elements of the $\\texttt{GPyTorch}$ API as required, commenting on their purpose.  The objective is to highlight scale, and specifically HPC scale.\n",
    "\n",
    "We will use data from a climate science application which was kindly shared with us by Julie Bessac (ANL/Virginia Tech), Vishwas Rao (ANL), Brandi Gamelin (ANL), and Tiffany Christian (Northwestern). The data comes from a climate simulation, which output a measure of drought called Standard Vapor Pressure Deficit Index (SVDI) at 12 km resolution and 1 day cadence over the continental United States (CONUS). The data has been collected into contiguous \"tiles\" of 16x16 points.\n",
    "\n",
    "We will start with a simple time-series regression, to familiarize ourselves with the $\\texttt{GPyTorch}$ interface. First, a few obligatory imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need PyTorch and GPyTorch\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import gpytorch\n",
    "# Gaussian processes really don't work well without 64-bit precision\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# And a few more players\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "input_data_all = np.load(\"/lus/eagle/projects/datascience/cgraziani/Drought_data/droughtSVDI_data_input.npy\")\n",
    "output_data_all = np.load(\"/lus/eagle/projects/datascience/cgraziani/Drought_data/droughtSVDI_data_ouput.npy\")\n",
    "\n",
    "print (input_data_all.shape, output_data_all.shape)\n",
    "# 365 days (one year), 18 N-S * 39 W-E tiles, each 16*16 points.\n",
    "# The input data codes latitude and longitude.\n",
    "# The output data codes SVDI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple time-series prediction\n",
    "We choose a tile from the data. In this case it's the 12th N-S tile and 30th W-E tile (no particular reason).  We chose the point (8,8) within that tile to extract a time-series of SVDI. The time series begins at day 10. We will train for 120 days of data. Then we will extend a window 30 days past the training window to see how well we can forecast SVDI.  \n",
    "\n",
    "First, prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easiest case: 1-D regression-prediction\n",
    "long=12 ; lat=30 ; point = (8,8)\n",
    "Start_Day=10 ; N_Train = 120\n",
    "\n",
    "# note: input locations are vectors even if they are 1-dimensional\n",
    "train_x = np.arange(Start_Day, Start_Day+N_Train, dtype=np.float64).reshape((N_Train,1)) \n",
    "train_x = Tensor(train_x)\n",
    "\n",
    "train_y=output_data_all[Start_Day:Start_Day+N_Train, long, lat, point[0], point[1]]\n",
    "train_y = np.array(train_y)\n",
    "train_y = Tensor(train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prepare the gpytorch model. There is a detailed explanation of the structure of this model at [the gpytorch documentation page](https://docs.gpytorch.ai/en/stable/index.html) and an annotated worked example at their [regression tutorial](https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/Simple_GP_Regression.html). This example is very similar to that one. It differs in the choice of GP kernel --- here we choose the Matern kernel, instead of the squared-exponential (AKA \"RBF\") kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        # A GP is defined by a mean function and a covariance function,\n",
    "        #\n",
    "        # Here is the mean\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        # And here is the covariance\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu = 2.5))\n",
    "        \n",
    " \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# The likelihood provides the model for any noise to be added to the model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "# The model instance.\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to set up for training. We need an optimizer. Surprisingly, Adam tends to work well, even though the parameter spaces are not very high-dimensional, $d=4$, vs. the $d=10^6$ tunable weight parameter spaces common in neural networks. We print out parameter values as we train. The way to discover the attributes associated with these is documented [here](https://docs.gpytorch.ai/en/stable/examples/00_Basic_Usage/Hyperparameters.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "N_iterations = 500\n",
    "def train():\n",
    "    for i in range(N_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward() \n",
    "        if i%10 == 0:\n",
    "            print(\"Iteration Number: %d\" % i)\n",
    "            print(\"Loss = %f\\n\" % loss)\n",
    "        optimizer.step()\n",
    "\n",
    "train()\n",
    "\n",
    "print(\"Noise: %f \" % model.likelihood.noise_covar.noise.item())\n",
    "print(\"Length Scale: %f \" %  model.covar_module.base_kernel.lengthscale.item())\n",
    "print(\"Output Scale: %f \" %  model.covar_module.outputscale.item())\n",
    "print(\"Mean:  %f \" % model.mean_module.constant.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the \"Length Scale\" is the correlation length (in days) learned from the time series. A value of about a day does not bode well for our ability to predict SVDI very far into the future! But let's take a look. It's prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Days = N_Train + 30\n",
    "Start_Test = Start_Day\n",
    "test_x = np.arange(Start_Test, Start_Test+Test_Days, dtype=np.float64).reshape((Test_Days,1))\n",
    "test_x = Tensor(test_x)\n",
    "test_y = output_data_all[Start_Test:Start_Test+Test_Days, long, lat, point[0], point[1]]\n",
    "test_y = np.array(test_y)\n",
    "test_y = Tensor(test_y)\n",
    "\n",
    "# Switch to predictive distribution\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    preds = likelihood(model(test_x)) #or model(test_x)\n",
    "\n",
    "lower, upper = preds.confidence_region() # 2-sigma confidence region\n",
    "\n",
    "fig1 = plt.figure()\n",
    "fig1.set_figwidth(8.0)\n",
    "fig1.set_figheight(8.0)\n",
    "ax = fig1.add_subplot(1,1,1)\n",
    "\n",
    "ax.plot(test_x.numpy(),test_y.numpy(), \"bo\")\n",
    "ax.plot(test_x.numpy(), preds.mean.numpy(), \"r-\")\n",
    "ax.fill_between(test_x.flatten().numpy(), lower.numpy(), upper.numpy(), alpha = 0.5)\n",
    "ax.axvline(x=Start_Day + N_Train)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red line is the mean of the predictive gaussian, the blue shaded area is the $2 \\sigma$ confidence region produced by the model.\n",
    "\n",
    "Well, this data sampled at this cadence is a tough modeling assignment. Even in the training region the model is forced to play connect-the-dots, because of the very noisy behavior of the data. In the forecasting region beyond day 120, we can see the effect of the very short ($<1$ day) length scale, which forces the model to its constant prior state in less than a day. Note, however, that as a *probabilistic* forecast the model did quite well, since the $2\\sigma$ ($\\sim$ 95,5% probability) contour does in fact bracket all the events post-Day 120.\n",
    "\n",
    "Possible exercises:\n",
    "\n",
    "1. What happens if you change the parameter for the GP kernel? A Matern kernel with `nu=1.5` instead of `nu=2.5` captures much \"rougher\" functions.  What do you see?\n",
    "\n",
    "3. What happens if you change the GP kernel type entirely, e.g. to a Radial Basis Function (RBF) kernel or a periodic kernel? A Matern kernel with $\\nu=\\infty$ is equivalent to the RBF kernel. \n",
    "\n",
    "2. Would you expect to obtain better predictions if you trained on a longer stretch of data? Try it! Explain what you found. (Hint: would the length scale change?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was an illustration of the use of GPyTorch, but it did not warm up the computers very much. Let's try harder.\n",
    "\n",
    "GPs are often used as interpolators. A common and useful application of interpolation in geostatistics is \"downscaling\".  In downscaling, one interpolates a higher-resolution spatial distribution from a lower one.\n",
    "\n",
    "Here we will take an $N\\times N$ array of tiles, each containing $16\\times 16$ points, and downscale the SVDI function on that array. First we will train a GP model to the resulting set of $256 \\times N^2$ values. This will bring the [KISS-GP](http://proceedings.mlr.press/v37/wilson15.html) algorithm into play: this is a so-called \"inducing point method\" that chooses a grid on which to interpolate the data so as to minimize error, while enabling fast Fourier transform methods to speed up the GP training operations that would otherwise cost $\\mathcal{O}(N^3)$ in time. The cost now scales as $m\\log m$ where $m$ is the grid size (for each dimension).\n",
    "\n",
    "Following training, we will interpolate the grid by predicting the SVDI values on a $(2N)\\times(2N)$ grid of points in the same region, using the standard GP regression technique. This will highlight the [LOVE](https://proceedings.mlr.press/v80/pleiss18a.html) (\"Lanczos Variance Estimates\") algorithm, which serves to remove the residual $\\mathcal{O}(N)$ cost of computation of predictive covariances. It is the combination of KISS-GP and LOVE that allows $\\texttt{GPyTorch}$ to claim \"constant in time\" scaling for GP modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep...\n",
    "#\n",
    "Num_Tiles = 8 # This is the square of tiles that we will interpolate\n",
    "Start_Lat = 4; Start_Long = 14\n",
    "Day_Number = 100\n",
    "\n",
    "# Extract the input data\n",
    "tiles_x = input_data_all[Day_Number, Start_Lat:Start_Lat+Num_Tiles, Start_Long:Start_Long+Num_Tiles, :, :, :]\n",
    "\n",
    "# Move the two latitude axes and the two longitude axes to make them respectively\n",
    "# adjacent, so that when we reshape the array they go together\n",
    "tiles_x = np.swapaxes(tiles_x, 1, 2)  # shape: (Num_Tiles, 16, Num_Tiles, 16, 2)\n",
    "print(\"tiles_x shape:\", tiles_x.shape)\n",
    "\n",
    "n_train = np.array(tiles_x.shape[:4]).prod() \n",
    "\n",
    "train_x = tiles_x.reshape((n_train,-1)) # shape: (Num_Tiles**2 * 256 , 2)\n",
    "print(\"train_x shape: \", train_x.shape)\n",
    "train_x = Tensor(train_x)\n",
    "\n",
    "# Now the output data\n",
    "tiles_y = output_data_all[Day_Number, Start_Lat:Start_Lat+Num_Tiles, Start_Long:Start_Long+Num_Tiles, :, :] # shape: (Num_Tiles, 16, Num_Tiles, 16)\n",
    "\n",
    "# Again, reorder the axes.\n",
    "tiles_y = np.swapaxes(tiles_y, 1, 2)\n",
    "print(\"tiles_y shape\", tiles_y.shape)\n",
    "\n",
    "train_y = tiles_y.reshape(n_train) # shape: (Num_Tiles**2 * 256)\n",
    "print(\"train_y shape:\", train_y.shape)\n",
    "train_y = Tensor(train_y) # shape: (Num_Tiles**2 * 256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the model again. Note the lines activating the KISS-GP algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "\n",
    "        # This is the kernel that will be wrapped by KISS-GP\n",
    "        self.base_covar_module = gpytorch.kernels.MaternKernel(nu = 2.5)\n",
    "\n",
    "        # The following two parameters are needed by KISS-GP\n",
    "        num_dims = train_x.shape[-1]\n",
    "        # A convenience function for computing appropriate grid sizes for this dataset\n",
    "        grid_size = gpytorch.utils.grid.choose_grid_size(train_x) \n",
    "\n",
    "        # This wrapper is only the output_scale\n",
    "        self.covar_module =gpytorch.kernels.ScaleKernel(\n",
    "                \n",
    "                # This wrapper, however, is KISS-GP\n",
    "                gpytorch.kernels.GridInterpolationKernel(self.base_covar_module, grid_size=grid_size, num_dims=num_dims)\n",
    "            )\n",
    "        \n",
    "     \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, on to training. Note that we have a much bigger training set now --- big enough that we might not even have attempted it a year or two ago!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################CUDA######################\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()\n",
    "    train_x, train_y = train_x.cuda(), train_y.cuda()\n",
    "######################\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.1)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "N_iterations = 81\n",
    "def train():\n",
    "    for i in range(N_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward() \n",
    "        if i%10 == 0:\n",
    "            print(\"Iteration Number: %d\" % i)\n",
    "            print(\"Loss = %f\\n\" % loss)\n",
    "        optimizer.step()\n",
    "#         scheduler.step(loss)\n",
    "\n",
    "train()\n",
    "print(\"Noise: %f \" % model.likelihood.noise_covar.noise.item())\n",
    "print(\"Length Scale: %f \" %  model.base_covar_module.lengthscale.item())\n",
    "print(\"Output Scale: %f \" %  model.covar_module.outputscale.item())\n",
    "print(\"Mean:  %f \" % model.mean_module.constant.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training could have gone on, but life is short.  Let's see how we do with the current model.\n",
    "\n",
    "We need an output tensor array to hold a $(2N)\\times(2N)$ grid of locations that we can interpolate to. This requires some explanation:\n",
    "\n",
    "[An explanation of the interpolation will be provided in a future version, with a picture, that hopefully helps to justify some of the possibly inscrutable numpy axis gymnastics+broadcasting+arithmetic+reshaping below.  It's geometrically sensible, but staring at the arrays will likely give you a migraine. Sorry about that.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To each original grid point there corresponds, in the new, interpolated grid,\n",
    "# 4 new points, one each to the NE, SE, SW, and NW. The x and y vector\n",
    "# components of the displacement from the original point of the new points all\n",
    "# have magnitude equal to 1/4 of the length of the side of a cell.\n",
    "#\n",
    "# That's what the following enigmatic numpy somehow accomplishes.\n",
    "\n",
    "dlat = tiles_x[0,0,0,0,0] - tiles_x[0,0,1,0,0] # input tile cell increment of latitude\n",
    "dlong = tiles_x[0,0,0,1,1] - tiles_x[0,0,0,0,1] # input tile cell increment of longitude\n",
    "\n",
    "test_tiles = np.expand_dims(tiles_x, axis=(2,5))\n",
    "\n",
    "delta_vecs = np.zeros((1,1,2,1,1,2,2))\n",
    "delta_vecs[0,0,0,0,0,0,0] = +dlat/4 ; delta_vecs[0,0,0,0,0,0,1] = -dlong/4\n",
    "delta_vecs[0,0,1,0,0,0,0] = -dlat/4 ; delta_vecs[0,0,1,0,0,0,1] = -dlong/4\n",
    "delta_vecs[0,0,0,0,0,1,0] = +dlat/4 ; delta_vecs[0,0,0,0,0,1,1] = +dlong/4\n",
    "delta_vecs[0,0,1,0,0,1,0] = -dlat/4 ; delta_vecs[0,0,1,0,0,1,1] = +dlong/4\n",
    "\n",
    "test_tiles = test_tiles + delta_vecs\n",
    "\n",
    "ts = np.array(test_tiles.shape) # shape: (Num_Tiles, 16, 2, Num_Tiles, 16, 2, 2)\n",
    "\n",
    "test_tiles_shape = [ts[0],ts[1]*ts[2], ts[3],ts[4]*ts[5],ts[6]]\n",
    "test_tiles = test_tiles.reshape(test_tiles_shape) \n",
    "# shape: (Num_Tiles, 32, Num_Tiles, 32, 2)\n",
    "\n",
    "n_test = np.array(test_tiles.shape[:4]).prod()\n",
    "test_x = test_tiles.reshape((n_test,-1)) # shape: (Num_Tiles**2 * 1024, 2)\n",
    "test_x = Tensor(test_x)\n",
    "if use_cuda:\n",
    "    test_x = test_x.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid in hand, we can now go into prediction mode. LOVE is activated using a python context as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var(): #LOVE is enabled\n",
    "    preds_kiss = likelihood(model(test_x)) #or model(test_x)\n",
    "    preds_mean = preds_kiss.mean\n",
    "    preds_var = preds_kiss.variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how did we do? Let's look at the results, side-by-side with the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odim = 16 * Num_Tiles\n",
    "ddim = 2 * odim\n",
    "downscaled_SVDI = preds_mean.cpu().numpy().reshape((ddim,ddim)).T\n",
    "orig_SVDI = train_y.cpu().numpy().reshape((odim,odim)).T\n",
    "\n",
    "long0 = tiles_x[0,0,0,0,1] ; long1 = tiles_x[0,0,-1,-1,1]\n",
    "lat0 = tiles_x [-1,-1,0,0,0] ; lat1 = tiles_x[0,0,0,0,0]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(17.0)\n",
    "fig.set_figheight(8.0)\n",
    "\n",
    "ax1 = fig.add_axes((0.1, 0.1, 0.45, 0.8))\n",
    "im1 = ax1.imshow(orig_SVDI, cmap=cm.RdYlGn, extent=(long0,long1,lat0,lat1))\n",
    "ax1.tick_params(axis=\"both\", which=\"both\", labelsize=16)\n",
    "ax1.set_title(\"Original SVDI\", fontsize=20)\n",
    "\n",
    "ax2 = fig.add_axes((0.6, 0.1, 0.45, 0.8))\n",
    "im2 = ax2.imshow(downscaled_SVDI, cmap=cm.RdYlGn, extent=(long0,long1,lat0,lat1))\n",
    "ax2.tick_params(axis=\"both\", which=\"both\", labelsize=16)\n",
    "ax2.set_title(\"Downscaled SVDI\", fontsize=20)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpolated version definitely looks as if it has higher resolution, so we can claim a success on that score there.\n",
    "\n",
    "Some possible exercises:\n",
    "\n",
    "1. Plot the root-variances as colormaps (better put up a colorbar). Can we say anything useful about the interpolation errors?  \n",
    "\n",
    "2. Does anything interesting happen if you change the kernel, dialing the assumed smoothness of the data up or down? \n",
    "\n",
    "3. Try increasing `Num_Tiles`. What do you observe about performance on the GPUs for the training and the predictions phases?  (*In a future version we'll have timers in this thing.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A future version of the tutorial will showcase an interesting capability of $\\texttt{GPyTorch}$: batch processing of independent GP fits. We will use the same dataset to fit GP models to all the tiles simultaneously, extracting the Length_Scale and Output_Scale parameters, and visualizing them on a map of the US. One such map, produced using $\\texttt{R}$ by Tiffany Christian, looks like this:\n",
    "\n",
    "![SVDI Correlation Lengths](./images/1982_gp.jpeg)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "b49f1ec3a14e2be89b8adfd319e91c8c719f2beadeb65bb4fc8e208ddbfe6dce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
